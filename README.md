# Logistic Regression from Scratch using NumPy

A complete implementation of logistic regression for binary classification built entirely from scratch using only NumPy. Bridging the gap between linear regression and neural networks.

## ðŸŽ¯ Project Overview

This project implements logistic regression from mathematical fundamentals:
- Sigmoid activation function
- Cross-entropy loss function
- Gradient descent for classification
- Decision boundary visualization
- Comprehensive binary classification metrics


## ðŸš€ Features

- âœ… Complete logistic regression implementation
- âœ… Sigmoid activation function from scratch
- âœ… Cross-entropy loss computation
- âœ… Gradient descent optimization
- âœ… Probability predictions and binary classification
- âœ… Decision boundary visualization (2D data)
- âœ… Classification metrics (accuracy, precision, recall, F1-score)

## ðŸ“Š Results

- Accuracy = 1.000
- Precision = 1.000
- Recall = 1.000
- F1 = 1.000

##  Bridge to Neural Networks

This implementation serves as a foundation for understanding:
- How sigmoid activation works in neural networks
- Cross-entropy loss in deep learning
- Gradient-based optimization
- The connection between logistic regression and neural networks
